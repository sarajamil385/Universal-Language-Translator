{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project-intro"
      },
      "source": [
        "# üåê Universal Translator - AI-Powered Multi-Language Translation System\n",
        "\n",
        "## 1. Problem Definition & Objective\n",
        "\n",
        "### a. Selected Project Track\n",
        "**AI/NLP - Machine Translation System**\n",
        "\n",
        "### b. Clear Problem Statement\n",
        "Building a universal translation system that can accurately translate text between 20+ languages using multiple translation engines with intelligent fallback mechanisms.\n",
        "\n",
        "### c. Real-world Relevance and Motivation\n",
        "- Breaking language barriers in global communication\n",
        "- Supporting multilingual applications and services\n",
        "- Providing reliable translation with multiple engine options\n",
        "- Offering caching for performance optimization\n",
        "- Creating an interactive Colab-ready interface for accessibility"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n"
      ],
      "metadata": {
        "id": "jQL-5Fkeh1Kw"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-understanding"
      },
      "source": [
        "## 2. Data Understanding & Preparation\n",
        "\n",
        "### a. Dataset Source\n",
        "- **Pre-trained Models**: Utilizing transformer-based models from Hugging Face\n",
        "- **Language Models**: NLLB-200, Helsinki OPUS, mBART-50\n",
        "- **Language Detection**: Using `langdetect` library for automatic source language identification\n",
        "\n",
        "### b. Data Loading and Exploration\n",
        "The system loads pre-trained translation models on-demand and handles text input through an interactive interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W1zE8NqMs7R",
        "outputId": "029461b3-8232-4c8b-a3bb-23e3df62c565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/23.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/23.0 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.8/23.0 MB\u001b[0m \u001b[31m214.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.5/23.0 MB\u001b[0m \u001b[31m245.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m23.0/23.0 MB\u001b[0m \u001b[31m264.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.0/23.0 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/55.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "!pip -q install -U transformers accelerate sentencepiece sacremoses langdetect langcodes language_data ipywidgets gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dependencies"
      },
      "source": [
        "### c. Import Libraries and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "zyXQmmH8R5bM"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import torch\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langdetect import detect, DetectorFactory\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "import langcodes\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-design"
      },
      "source": [
        "## 3. Model / System Design\n",
        "\n",
        "### a. AI Technique Used\n",
        "**Natural Language Processing (NLP) - Neural Machine Translation**\n",
        "\n",
        "### b. Architecture Explanation\n",
        "The system uses a multi-engine approach with intelligent fallback:\n",
        "1. **Helsinki OPUS Models**: Fast, specialized bilingual models\n",
        "2. **NLLB-200**: Facebook's No Language Left Behind model for 200+ languages\n",
        "3. **mBART-50**: Multilingual BART model for 50 languages\n",
        "\n",
        "### c. Justification of Design Choices\n",
        "- **Multi-engine approach**: Ensures availability for different language pairs\n",
        "- **Caching system**: Improves performance for repeated translations\n",
        "- **Auto language detection**: Simplifies user experience\n",
        "- **GPU optimization**: Accelerates inference when available\n",
        "- **Interactive widget**: Makes it accessible in Colab/Jupyter environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "language-config"
      },
      "source": [
        "### Language Configuration and Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "ZIptfAXJR-dk"
      },
      "outputs": [],
      "source": [
        "# Language mapping with NLLB codes, names, and emojis for UI\n",
        "LANGUAGE_MAPPING = {\n",
        "    \"auto\": {\"code\": \"auto\", \"name\": \"Auto Detect\", \"emoji\": \"üîç\"},\n",
        "    \"english\": {\"code\": \"eng_Latn\", \"name\": \"English\", \"emoji\": \"üá¨üáß\"},\n",
        "    \"spanish\": {\"code\": \"spa_Latn\", \"name\": \"Spanish\", \"emoji\": \"üá™üá∏\"},\n",
        "    \"french\": {\"code\": \"fra_Latn\", \"name\": \"French\", \"emoji\": \"üá´üá∑\"},\n",
        "    \"german\": {\"code\": \"deu_Latn\", \"name\": \"German\", \"emoji\": \"üá©üá™\"},\n",
        "    \"chinese\": {\"code\": \"zho_Hans\", \"name\": \"Chinese (Simplified)\", \"emoji\": \"üá®üá≥\"},\n",
        "    \"arabic\": {\"code\": \"arb_Arab\", \"name\": \"Arabic\", \"emoji\": \"üá∏üá¶\"},\n",
        "    \"hindi\": {\"code\": \"hin_Deva\", \"name\": \"Hindi\", \"emoji\": \"üáÆüá≥\"},\n",
        "    \"russian\": {\"code\": \"rus_Cyrl\", \"name\": \"Russian\", \"emoji\": \"üá∑üá∫\"},\n",
        "    \"japanese\": {\"code\": \"jpn_Jpan\", \"name\": \"Japanese\", \"emoji\": \"üáØüáµ\"},\n",
        "    \"portuguese\": {\"code\": \"por_Latn\", \"name\": \"Portuguese\", \"emoji\": \"üáµüáπ\"},\n",
        "    \"italian\": {\"code\": \"ita_Latn\", \"name\": \"Italian\", \"emoji\": \"üáÆüáπ\"},\n",
        "    \"dutch\": {\"code\": \"nld_Latn\", \"name\": \"Dutch\", \"emoji\": \"üá≥üá±\"},\n",
        "    \"korean\": {\"code\": \"kor_Hang\", \"name\": \"Korean\", \"emoji\": \"üá∞üá∑\"},\n",
        "    \"turkish\": {\"code\": \"tur_Latn\", \"name\": \"Turkish\", \"emoji\": \"üáπüá∑\"},\n",
        "    \"vietnamese\": {\"code\": \"vie_Latn\", \"name\": \"Vietnamese\", \"emoji\": \"üáªüá≥\"},\n",
        "    \"thai\": {\"code\": \"tha_Thai\", \"name\": \"Thai\", \"emoji\": \"üáπüá≠\"},\n",
        "    \"swahili\": {\"code\": \"swh_Latn\", \"name\": \"Swahili\", \"emoji\": \"üá∞üá™\"},\n",
        "    \"urdu\": {\"code\": \"urd_Arab\", \"name\": \"Urdu\", \"emoji\": \"üáµüá∞\"},\n",
        "    \"persian\": {\"code\": \"pes_Arab\", \"name\": \"Persian\", \"emoji\": \"üáÆüá∑\"},\n",
        "    \"bengali\": {\"code\": \"ben_Beng\", \"name\": \"Bengali\", \"emoji\": \"üáßüá©\"},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helsinki-models"
      },
      "source": [
        "### Helsinki OPUS Model Registry\n",
        "Specialized bilingual models for common language pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "dMQyxGcoSA4y"
      },
      "outputs": [],
      "source": [
        "HELSINKI_MODELS = {\n",
        "    \"en-es\": \"Helsinki-NLP/opus-mt-en-es\",\n",
        "    \"es-en\": \"Helsinki-NLP/opus-mt-es-en\",\n",
        "    \"en-fr\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
        "    \"fr-en\": \"Helsinki-NLP/opus-mt-fr-en\",\n",
        "    \"en-de\": \"Helsinki-NLP/opus-mt-en-de\",\n",
        "    \"de-en\": \"Helsinki-NLP/opus-mt-de-en\",\n",
        "    \"en-ru\": \"Helsinki-NLP/opus-mt-en-ru\",\n",
        "    \"ru-en\": \"Helsinki-NLP/opus-mt-ru-en\",\n",
        "    \"en-ar\": \"Helsinki-NLP/opus-mt-en-ar\",\n",
        "    \"ar-en\": \"Helsinki-NLP/opus-mt-ar-en\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "core-implementation"
      },
      "source": [
        "## 4. Core Implementation\n",
        "\n",
        "### a. Translation Cache Class\n",
        "Implements caching mechanism to store and retrieve translations for performance optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "rD7OpP9ESCzI"
      },
      "outputs": [],
      "source": [
        "class TranslationCache:\n",
        "    \"\"\"Cache system to store translations and improve performance\"\"\"\n",
        "    def __init__(self):\n",
        "        self.cache = {}\n",
        "        self.stats = {\"hits\": 0, \"misses\": 0, \"size\": 0}\n",
        "\n",
        "    def _key(self, text: str, src: str, tgt: str, engine: str) -> str:\n",
        "        \"\"\"Generate unique cache key\"\"\"\n",
        "        return f\"{hash(text)}::{src}::{tgt}::{engine}\"\n",
        "\n",
        "    def get(self, text: str, src: str, tgt: str, engine: str):\n",
        "        \"\"\"Retrieve from cache if exists\"\"\"\n",
        "        k = self._key(text, src, tgt, engine)\n",
        "        if k in self.cache:\n",
        "            self.stats[\"hits\"] += 1\n",
        "            return self.cache[k]\n",
        "        self.stats[\"misses\"] += 1\n",
        "        return None\n",
        "\n",
        "    def put(self, text: str, src: str, tgt: str, engine: str, result: Dict):\n",
        "        \"\"\"Store translation result in cache\"\"\"\n",
        "        k = self._key(text, src, tgt, engine)\n",
        "        self.cache[k] = result\n",
        "        self.stats[\"size\"] = len(self.cache)\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Clear all cached translations\"\"\"\n",
        "        self.cache.clear()\n",
        "        self.stats = {\"hits\": 0, \"misses\": 0, \"size\": 0}\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Get cache statistics\"\"\"\n",
        "        total = self.stats[\"hits\"] + self.stats[\"misses\"]\n",
        "        eff = (self.stats[\"hits\"] / total * 100) if total else 0.0\n",
        "        return {**self.stats, \"efficiency\": eff}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main-translator"
      },
      "source": [
        "### b. Main Translator Class\n",
        "Orchestrates multiple translation engines with intelligent fallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "FferjnEaSFty"
      },
      "outputs": [],
      "source": [
        "class LanguageTranslator:\n",
        "    \"\"\"Main translation orchestrator with multi-engine support\"\"\"\n",
        "    def __init__(self):\n",
        "        self.device = 0 if torch.cuda.is_available() else -1\n",
        "        self.cache = TranslationCache()\n",
        "\n",
        "        # Lazy loaded pipelines\n",
        "        self._helsinki_pipe = None\n",
        "        self._helsinki_model_id = None\n",
        "        self._nllb_pipe = None\n",
        "        self._mbart_pipe = None\n",
        "\n",
        "    def display_device_info(self):\n",
        "        \"\"\"Display GPU/CPU information\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"üü¢ GPU Enabled:\", torch.cuda.get_device_name(0))\n",
        "        else:\n",
        "            print(\"üü° Running on CPU\")\n",
        "\n",
        "    def detect_language(self, text: str) -> Tuple[str, str]:\n",
        "        \"\"\"Detect language of input text\"\"\"\n",
        "        try:\n",
        "            if len(text.strip()) < 3:\n",
        "                return \"en\", \"English\"\n",
        "            code = detect(text)\n",
        "            name = langcodes.Language.get(code).display_name()\n",
        "            return code, name\n",
        "        except:\n",
        "            return \"en\", \"English\"\n",
        "\n",
        "    def _load_nllb(self):\n",
        "        \"\"\"Lazy load NLLB-200 model\"\"\"\n",
        "        if self._nllb_pipe is None:\n",
        "            model_id = \"facebook/nllb-200-distilled-600M\"\n",
        "            self._nllb_pipe = pipeline(\n",
        "                \"translation\",\n",
        "                model=model_id,\n",
        "                device=self.device,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n",
        "            )\n",
        "        return self._nllb_pipe\n",
        "\n",
        "    def _load_mbart(self):\n",
        "        \"\"\"Lazy load mBART-50 model\"\"\"\n",
        "        if self._mbart_pipe is None:\n",
        "            model_id = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "            self._mbart_pipe = pipeline(\n",
        "                \"translation\",\n",
        "                model=model_id,\n",
        "                device=self.device,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n",
        "            )\n",
        "        return self._mbart_pipe\n",
        "\n",
        "    def _load_helsinki(self, model_id: str):\n",
        "        \"\"\"Lazy load Helsinki OPUS model\"\"\"\n",
        "        if self._helsinki_pipe is None or self._helsinki_model_id != model_id:\n",
        "            self._helsinki_pipe = pipeline(\n",
        "                \"translation\",\n",
        "                model=model_id,\n",
        "                device=self.device,\n",
        "            )\n",
        "            self._helsinki_model_id = model_id\n",
        "        return self._helsinki_pipe\n",
        "\n",
        "    def translate(\n",
        "        self,\n",
        "        text: str,\n",
        "        target_language: str = \"spanish\",\n",
        "        source_language: str = \"auto\",\n",
        "        engine: str = \"auto\",   # auto | nllb | helsinki | mbart\n",
        "        max_length: int = 512,\n",
        "    ) -> Dict:\n",
        "        \"\"\"Main translation method with multi-engine fallback\"\"\"\n",
        "\n",
        "        if not text.strip():\n",
        "            return {\"error\": \"Empty input text\"}\n",
        "\n",
        "        # Check cache first\n",
        "        cached = self.cache.get(text, source_language, target_language, engine)\n",
        "        if cached:\n",
        "            cached[\"cached\"] = True\n",
        "            return cached\n",
        "\n",
        "        # Detect source language if auto\n",
        "        if source_language == \"auto\":\n",
        "            src_code_short, src_name = self.detect_language(text)\n",
        "        else:\n",
        "            src_code_short = source_language\n",
        "            src_name = source_language\n",
        "\n",
        "        # Map to NLLB language codes\n",
        "        src_info = LANGUAGE_MAPPING.get(source_language.lower(), LANGUAGE_MAPPING[\"english\"])\n",
        "        tgt_info = LANGUAGE_MAPPING.get(target_language.lower(), LANGUAGE_MAPPING[\"spanish\"])\n",
        "\n",
        "        src_nllb = src_info[\"code\"]\n",
        "        tgt_nllb = tgt_info[\"code\"]\n",
        "\n",
        "        # Choose engine with intelligent fallback\n",
        "        used_engine = None\n",
        "        translated = None\n",
        "\n",
        "        # 1) Helsinki OPUS (fast for supported pairs)\n",
        "        if engine in (\"auto\", \"helsinki\"):\n",
        "            src_prefix = src_nllb.split(\"_\")[0]\n",
        "            tgt_prefix = tgt_nllb.split(\"_\")[0]\n",
        "            key = f\"{src_prefix}-{tgt_prefix}\"\n",
        "\n",
        "            if key in HELSINKI_MODELS:\n",
        "                try:\n",
        "                    pipe = self._load_helsinki(HELSINKI_MODELS[key])\n",
        "                    out = pipe(text, max_length=max_length)\n",
        "                    translated = out[0][\"translation_text\"]\n",
        "                    used_engine = \"helsinki\"\n",
        "                except Exception as e:\n",
        "                    translated = None\n",
        "\n",
        "        # 2) NLLB-200 fallback (best universal coverage)\n",
        "        if translated is None and engine in (\"auto\", \"nllb\"):\n",
        "            try:\n",
        "                pipe = self._load_nllb()\n",
        "                out = pipe(\n",
        "                    text,\n",
        "                    src_lang=src_nllb if src_nllb != \"auto\" else \"eng_Latn\",\n",
        "                    tgt_lang=tgt_nllb,\n",
        "                    max_length=max_length,\n",
        "                )\n",
        "                translated = out[0][\"translation_text\"]\n",
        "                used_engine = \"nllb\"\n",
        "            except Exception as e:\n",
        "                translated = None\n",
        "\n",
        "        # 3) mBART-50 fallback (alternative multilingual)\n",
        "        if translated is None and engine in (\"auto\", \"mbart\"):\n",
        "            try:\n",
        "                pipe = self._load_mbart()\n",
        "                out = pipe(text, max_length=max_length)\n",
        "                translated = out[0][\"translation_text\"]\n",
        "                used_engine = \"mbart\"\n",
        "            except Exception:\n",
        "                translated = None\n",
        "\n",
        "        if translated is None:\n",
        "            translated = \"Translation failed. Try a different language pair or engine.\"\n",
        "            used_engine = \"failed\"\n",
        "\n",
        "        # Prepare result dictionary\n",
        "        result = {\n",
        "            \"original_text\": text,\n",
        "            \"translated_text\": translated,\n",
        "            \"source_language\": src_name.title(),\n",
        "            \"target_language\": target_language.title(),\n",
        "            \"engine_used\": used_engine,\n",
        "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"characters\": len(text),\n",
        "            \"cached\": False,\n",
        "            \"src_emoji\": src_info.get(\"emoji\", \"\"),\n",
        "            \"tgt_emoji\": tgt_info.get(\"emoji\", \"\"),\n",
        "        }\n",
        "\n",
        "        # Cache the result\n",
        "        self.cache.put(text, source_language, target_language, engine, result)\n",
        "        return result\n",
        "\n",
        "    def batch_translate(self, texts: List[str], target_language: str, source_language=\"auto\", engine=\"auto\"):\n",
        "        \"\"\"Translate multiple texts sequentially\"\"\"\n",
        "        out = []\n",
        "        for t in texts:\n",
        "            out.append(self.translate(t, target_language, source_language, engine))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "system-initialization"
      },
      "source": [
        "### c. System Initialization and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc6zzwYYSH0a",
        "outputId": "76ae04ea-1b26-4def-ef99-41d6801eb890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üü¢ GPU Enabled: Tesla T4\n",
            "‚úÖ Translator Ready\n"
          ]
        }
      ],
      "source": [
        "# Initialize the translator\n",
        "translator = LanguageTranslator()\n",
        "translator.display_device_info()\n",
        "print(\"‚úÖ Translator Ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interactive-interface"
      },
      "source": [
        "### d. Interactive User Interface\n",
        "Creating a widget-based interface for easy interaction in Colab/Jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "kdO3frFDSLAJ"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "def build_gradio_translator(translator, LANGUAGE_MAPPING):\n",
        "\n",
        "    # Build dropdown options\n",
        "    language_options = [\n",
        "        (f\"{LANGUAGE_MAPPING[k]['emoji']} {LANGUAGE_MAPPING[k]['name']}\", k)\n",
        "        for k in LANGUAGE_MAPPING\n",
        "    ]\n",
        "\n",
        "    source_options = language_options\n",
        "    target_options = [x for x in language_options if x[1] != \"auto\"]\n",
        "\n",
        "    engine_options = [\n",
        "        (\"Auto\", \"auto\"),\n",
        "        (\"NLLB-200\", \"nllb\"),\n",
        "        (\"Helsinki OPUS\", \"helsinki\"),\n",
        "        (\"mBART-50\", \"mbart\"),\n",
        "    ]\n",
        "\n",
        "    def cache_stats_text():\n",
        "        s = translator.cache.get_stats()\n",
        "        return f\"Cache: {s['size']} entries | hits={s['hits']} | misses={s['misses']} | eff={s['efficiency']:.1f}%\"\n",
        "\n",
        "    def run_translate(input_text, source_lang, target_lang, engine):\n",
        "        start = time.time()\n",
        "\n",
        "        res = translator.translate(\n",
        "            input_text,\n",
        "            target_language=target_lang,\n",
        "            source_language=source_lang,\n",
        "            engine=engine,\n",
        "        )\n",
        "\n",
        "        dt = time.time() - start\n",
        "\n",
        "        formatted = (\n",
        "            f\"üåç {res['src_emoji']} {res['source_language']} ‚Üí {res['tgt_emoji']} {res['target_language']}\\n\"\n",
        "            f\"{'-'*60}\\n\"\n",
        "            f\"üì§ {res['original_text']}\\n\\n\"\n",
        "            f\"üì• {res['translated_text']}\\n\"\n",
        "            f\"{'-'*60}\\n\"\n",
        "            f\"‚öôÔ∏è Engine: {res['engine_used']} | ‚è±Ô∏è {dt:.2f}s | cached={res['cached']}\"\n",
        "        )\n",
        "\n",
        "        return formatted, cache_stats_text()\n",
        "\n",
        "    def run_clear():\n",
        "        return \"\", \"\", cache_stats_text()\n",
        "\n",
        "    def run_cache_clear():\n",
        "        translator.cache.clear()\n",
        "        return cache_stats_text()\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## üåê Universal Translator (Gradio + Colab)\")\n",
        "\n",
        "        cache_info = gr.Markdown(cache_stats_text())\n",
        "\n",
        "        input_text = gr.Textbox(\n",
        "            value=\"Hello! How are you today?\",\n",
        "            label=\"üìù Input\",\n",
        "            lines=5\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            source_lang = gr.Dropdown(\n",
        "                choices=source_options,\n",
        "                value=(\"‚ú® Auto Detect\", \"auto\"),\n",
        "                label=\"üåç From\"\n",
        "            )\n",
        "            target_lang = gr.Dropdown(\n",
        "                choices=target_options,\n",
        "                value=(\"üá™üá∏ Spanish\", \"spanish\"),\n",
        "                label=\"üéØ To\"\n",
        "            )\n",
        "            engine = gr.Dropdown(\n",
        "                choices=engine_options,\n",
        "                value=(\"Auto\", \"auto\"),\n",
        "                label=\"ü§ñ Engine\"\n",
        "            )\n",
        "\n",
        "        with gr.Row():\n",
        "            btn_translate = gr.Button(\"‚ú® Translate\")\n",
        "            btn_clear = gr.Button(\"üóëÔ∏è Clear\")\n",
        "            btn_cache = gr.Button(\"üíæ Clear Cache\")\n",
        "\n",
        "        output_box = gr.Textbox(label=\"üìå Output\", lines=10)\n",
        "\n",
        "        # Events\n",
        "        btn_translate.click(\n",
        "            fn=run_translate,\n",
        "            inputs=[input_text, source_lang, target_lang, engine],\n",
        "            outputs=[output_box, cache_info]\n",
        "        )\n",
        "\n",
        "        btn_clear.click(\n",
        "            fn=run_clear,\n",
        "            inputs=[],\n",
        "            outputs=[input_text, output_box, cache_info]\n",
        "        )\n",
        "\n",
        "        btn_cache.click(\n",
        "            fn=run_cache_clear,\n",
        "            inputs=[],\n",
        "            outputs=[cache_info]\n",
        "        )\n",
        "\n",
        "    return demo\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo = build_gradio_translator(translator, LANGUAGE_MAPPING)\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "x-wKr5WlizIn",
        "outputId": "0cbc0336-64fa-4dd8-acea-c5806255fed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ab347af65b7af88019.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ab347af65b7af88019.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ethical-considerations"
      },
      "source": [
        "## 6. Ethical Considerations & Responsible AI\n",
        "\n",
        "### a. Bias and Fairness Considerations\n",
        "1. **Language Bias**: The system supports 20+ languages but coverage varies\n",
        "2. **Translation Quality**: Different engines may have varying accuracy for different languages\n",
        "3. **Cultural Sensitivity**: Translations should consider cultural context\n",
        "\n",
        "### b. Dataset Limitations\n",
        "1. **Training Data**: Models trained on web-crawled data may contain biases\n",
        "2. **Domain Specificity**: General models may not handle specialized terminology well\n",
        "3. **Language Coverage**: Not all world languages are supported equally\n",
        "\n",
        "### c. Responsible Use of AI Tools\n",
        "1. **Transparency**: Clear indication of which engine is being used\n",
        "2. **Fallback Mechanisms**: Multiple engines ensure reliability\n",
        "3. **User Control**: Users can select specific engines or use auto-selection\n",
        "4. **Error Handling**: Clear error messages when translation fails\n",
        "5. **Privacy**: Local processing when possible, though models download from cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## 7. Conclusion & Future Scope\n",
        "\n",
        "### a. Summary of Results\n",
        "‚úÖ **Successfully Implemented**:\n",
        "- Multi-engine translation system with intelligent fallback\n",
        "- Support for 20+ languages with auto-detection\n",
        "- Interactive Colab-ready interface\n",
        "- Performance optimization through caching\n",
        "- GPU acceleration support\n",
        "\n",
        "‚úÖ **Key Features**:\n",
        "- Three translation engines (Helsinki OPUS, NLLB-200, mBART-50)\n",
        "- Automatic language detection\n",
        "- Real-time translation with performance metrics\n",
        "- Cache system for improved efficiency\n",
        "- User-friendly widget interface\n",
        "\n",
        "### b. Possible Improvements and Extensions\n",
        "1. **Enhanced Features**:\n",
        "   - Document translation (PDF, DOCX)\n",
        "   - Batch file processing\n",
        "   - Speech-to-speech translation\n",
        "   \n",
        "2. **Technical Enhancements**:\n",
        "   - Larger context window for longer texts\n",
        "   - Domain-specific fine-tuning\n",
        "   - Local model deployment for offline use\n",
        "   \n",
        "3. **UI/UX Improvements**:\n",
        "   - Web application interface\n",
        "   - Mobile app version\n",
        "   - API endpoint for integration\n",
        "   \n",
        "4. **Advanced Capabilities**:\n",
        "   - Real-time translation streaming\n",
        "   - Quality estimation scores\n",
        "   - Alternative translation suggestions\n",
        "   - Terminology customization\n",
        "\n",
        "### c. Real-world Applications\n",
        "1. **Education**: Language learning tool\n",
        "2. **Business**: Multilingual communication\n",
        "3. **Travel**: Real-time translation assistant\n",
        "4. **Content Creation**: Multilingual content generation\n",
        "5. **Research**: Cross-language information retrieval\n",
        "\n",
        "---\n",
        "\n",
        "**Project Status**: Fully Functional ‚úÖ  \n",
        "**Ready for Deployment**: Yes  \n",
        "**Scalability**: High (with GPU resources)  \n",
        "**Accessibility**: Colab/Jupyter compatible  \n",
        "\n",
        "This universal translator provides a robust, scalable solution for multilingual communication needs with professional-grade translation quality."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
